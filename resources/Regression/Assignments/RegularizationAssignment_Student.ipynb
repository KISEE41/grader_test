{"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"OP4RKcmxbYP2","nbgrader":{"cell_type":"markdown","checksum":"788f7611b1b6f2a8b789cf79357adb1a","grade":false,"grade_id":"cell-b6726afbd980198a","locked":true,"schema_version":3,"solution":false}},"source":["# Used Motorcycle Price Prediction\n","\n","In this assignment you will implement a number of linear regression models to predict the price of a used motorcycle given its different features and perform necessary evaluations to check the model performance.\n","\n","<b><div style=\"text-align: right\">[TOTAL POINTS: 15]</div></b>\n","\n","### Learning Objective\n","\n","* Test the assumptions of linear regression and transform the dataset accordingly if necessary.\n","\n","* Use the sklearn library to implement linear reression, ridge regression and lasso and evaluate their and calculate their $R^2$ score.\n","\n","* Implement and use the adjusted $R^2$ score to evaluate the performance of models with varying set of features.\n","\n","* Implement the Ordinary Least Squares and Gradient descent method from scratch to find the parameters of Linear Regression.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"kH4QfoxuB_tR","nbgrader":{"cell_type":"markdown","checksum":"527b088be65b96c46e169e9979d60fa1","grade":false,"grade_id":"cell-dd524d47186722b5","locked":true,"schema_version":3,"solution":false}},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"pkKSO1_5sTfJ","nbgrader":{"cell_type":"code","checksum":"c85d0b45cf12a40e115d6497a3d1ea93","grade":false,"grade_id":"cell-0493b09dd705f865","locked":true,"schema_version":3,"solution":false}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","import itertools\n","\n","np.random.seed(42)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"e9sDwvTjWuNR","nbgrader":{"cell_type":"markdown","checksum":"1e6e8beced2078ccee2f6be69ddc8c45","grade":false,"grade_id":"cell-b62805968ee2f11a","locked":true,"schema_version":3,"solution":false}},"source":["## Dataset Description\n","\n","The dataset contains 500 samples of used motorcycle with some of their features and their resale value. The problem here is to predict the resale value of the motorcycle using its features.\n","\n","**Number of Instances:** 500\n","\n","**Number of Attributes:** 5 (Input Features) + 1 (Target)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t398TZYeZqFy"},"source":["### Attribute Information\n","\n","#### **Input Features**\n","All the feature values have already been scaled to a similar range so that you won't have to apply feature scaling.\n","\n","- **Lot No.:** Lot number of the motorcycle. Higher Lot number signifies newer motorcycle.\n","\n","- **Engine Size:** Size of the eingine of the motorcycle.\n","\n","- **Mileage:** Measure of how long the motorcycle runs for a litre of gas.\n","\n","- **KM Run:** Kilometers run on the motorcycle.\n","\n","- **Max Power:** Maximum power output of the motorcycle.\n","\n","\\\n","\n","#### **Target Variable**\n","\n","- **Price:** Resale value of the motorcycle in thousands. Our target variable.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"ypY8Uj_BrMGe","nbgrader":{"cell_type":"code","checksum":"35cbd07f9173f989c05e52902f4fc959","grade":false,"grade_id":"cell-32991d83ab4a4932","locked":true,"schema_version":3,"solution":false}},"outputs":[],"source":["dataset = pd.read_csv('https://drive.google.com/uc?id=1zeE911284qKNF1q2mgoIgOa4UXYC9N2i')\n","X = dataset.drop('Price', axis=1).values\n","y = dataset['Price'].values.reshape(X.shape[0],1)\n","dataset.head()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"qSdqIHE7NXOL","nbgrader":{"cell_type":"markdown","checksum":"1b5d7583c220791c02d6fb7b8882f13b","grade":false,"grade_id":"cell-4f5bbe0ea558b7c8","locked":true,"schema_version":3,"solution":false}},"source":["Let's first calculate the residuals by quickly fitting the linear regression on the whole data using sklearn."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"x2LkJ1D795J8","nbgrader":{"cell_type":"code","checksum":"03f5caa1614eec7718321d48d247a69a","grade":false,"grade_id":"cell-ab008035339fa824","locked":true,"schema_version":3,"solution":false}},"outputs":[],"source":["## RUN THIS CELL\n","from sklearn.linear_model import LinearRegression\n","model = LinearRegression().fit(X, y)\n","residuals = model.predict(X) - y"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Imey1oOsxKAr","nbgrader":{"cell_type":"markdown","checksum":"f5bec1a0e95c4f5a6bc36dd1c7ab95cd","grade":false,"grade_id":"cell-783b34fd6fcc3a5a","locked":true,"schema_version":3,"solution":false}},"source":["## Part 1: Assumptions"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"9UE1xm7axKAs","nbgrader":{"cell_type":"markdown","checksum":"a31e806ac2fdd83250851f0aabc18d6f","grade":false,"grade_id":"cell-ae17146385b3104f","locked":true,"schema_version":3,"solution":false}},"source":["### Exercise 1.1: Check for multicollinearity\n","\n","\n","**<div style=\"text-align: right\"> [POINTS: 1]</div>**\n","\n","\n","**Task:**\n","\n","* Import the [`variance_inflation_factor`]() from `statsmodels` library.\n","\n","* Calculate the VIF score for each of the features using the `variance_inflation_factor` function."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"Uwu3IwwHxKAt","nbgrader":{"cell_type":"code","checksum":"637b3d369bfcf8ef6ecd61df7676c274","grade":false,"grade_id":"cell-422a17c733ef2dab","locked":false,"schema_version":3,"solution":true},"tags":["Ex-1-Task-1"]},"outputs":[],"source":["### Ex-1-Task-1\n","VIF = None\n","\n","### BEGIN SOLUTION\n","# your code here\n","raise NotImplementedError\n","### END SOLUTION\n","\n","print(VIF)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"75-785tsdqaG","nbgrader":{"cell_type":"code","checksum":"8de92ecd868f6c2d1f7b028f1f896449","grade":true,"grade_id":"cell-838fbd2f25fb97a2","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-1-Task-1"]},"outputs":[],"source":["assert VIF is not None\n","assert len(VIF) == X.shape[1]"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"z-qrRwrekDxf","nbgrader":{"cell_type":"markdown","checksum":"e49ee806698293cb15239ba797b620c6","grade":false,"grade_id":"cell-8084300e4ca45c51","locked":true,"schema_version":3,"solution":false}},"source":["**Removing Multicollinearity**"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"djB5bxT136bQ","nbgrader":{"cell_type":"code","checksum":"5f3427f7767dfc227c1ad34ae9455454","grade":false,"grade_id":"cell-ea2a420f1706a964","locked":true,"schema_version":3,"solution":false}},"outputs":[],"source":["### RUN THIS CELL\n","X = np.delete(X, 4, 1)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"FjFjcqpdxKAx","nbgrader":{"cell_type":"markdown","checksum":"0578fdb58976d1cb962732647f43882a","grade":false,"grade_id":"cell-d75c860758c47627","locked":true,"schema_version":3,"solution":false}},"source":["### Exercise 1.2: Check for Homoscedasticity\n","\n","**<div style=\"text-align: right\"> [POINTS: 1]</div>**\n","\n","**Task:**\n","- Assign the appropriate value for the `x_axis` and `y_axis` to plot the scatterplot of the samples (x-axis) and their corresponding residuals (y-axis)."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"bD9vlL8S9BFO","nbgrader":{"cell_type":"code","checksum":"45b5467c3b3d8a1cadbf7e6e386b1846","grade":false,"grade_id":"cell-3a8596acc63f6010","locked":false,"schema_version":3,"solution":true},"tags":["Ex-1-Task-2"]},"outputs":[],"source":["### Ex-1-Task-2\n","x_axis = None\n","y_axis = None\n","\n","### BEGIN SOLUTION\n","# your code here\n","raise NotImplementedError\n","### END SOLUTION\n","\n","plt.scatter(x=x_axis, y=y_axis)\n","plt.title('Variance of Residuals')\n","plt.xlabel(\"Fitted Value\")\n","plt.ylabel(\"Residual\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"dm8OUOB9-MBQ","nbgrader":{"cell_type":"code","checksum":"36ca24051c95426676311b8254be0250","grade":true,"grade_id":"cell-189542342fcbbbc3","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-1-Task-2"]},"outputs":[],"source":["assert max(x_axis) == 499\n","assert len(y_axis) == 500"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"M3fV0J4M-xEp","nbgrader":{"cell_type":"markdown","checksum":"0526651712fa3d58c338d6594e556d4a","grade":false,"grade_id":"cell-0dffa25e49b54843","locked":true,"schema_version":3,"solution":false}},"source":["### Exercise 1.3: Check for Auto-correlation\n","\n","**<div style=\"text-align: right\"> [POINTS: 1]</div>**\n","\n","\n","**Task:**\n","\n","* Import the [`durbin_watson`]() from `statsmodels` library.\n","\n","* Calculate the `durbin_watson_score` of the residuals using the `durbin_watson` function."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"7ubnbDlF-14b","nbgrader":{"cell_type":"code","checksum":"413d0bd3eedeccb2426e663f5f42d81c","grade":false,"grade_id":"cell-e9fefc6d6b5ab3eb","locked":false,"schema_version":3,"solution":true},"tags":["Ex-1-Task-3"]},"outputs":[],"source":["### Ex-1-Task-3\n","\n","durbin_watson_score = None\n","\n","### BEGIN SOLUTION\n","# your code here\n","raise NotImplementedError\n","### END SOLUTION\n","\n","print(durbin_watson_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"k4P-NXEUfmtN","nbgrader":{"cell_type":"code","checksum":"0000b9266f63fc501051f86b42afa795","grade":true,"grade_id":"cell-168402cfcfe3e21a","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-1-Task-3"]},"outputs":[],"source":["assert durbin_watson_score is not None\n","assert durbin_watson_score >= 1.5"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"G3jPbDgq_I-a","nbgrader":{"cell_type":"markdown","checksum":"e6202deb13293d0838ec921d75e4640f","grade":false,"grade_id":"cell-c3b84ef1f069c690","locked":true,"schema_version":3,"solution":false}},"source":["### Exercise 1.4: Check for Normality of residuals\n","\n","**<div style=\"text-align: right\"> [POINTS: 1]</div>**\n","\n","**Task:**\n","\n","* Import the [`normal_ad`]() from `statsmodels` library.\n","\n","* Calculate the `p_value` of the residuals using the `normal_ad` function."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"ix2c67dQ_JuA","nbgrader":{"cell_type":"code","checksum":"eed0f9af5c3bc8ed6f59a8ddf4f4ecfc","grade":false,"grade_id":"cell-94bf01ebb2bd64e9","locked":false,"schema_version":3,"solution":true},"tags":["Ex-1-Task-4"]},"outputs":[],"source":["### Ex-1-Task-4\n","\n","p_value = None\n","\n","### BEGIN SOLUTION\n","# your code here\n","raise NotImplementedError\n","### END SOLUTION\n","\n","print(p_value)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"V522d0KM5LMC","nbgrader":{"cell_type":"code","checksum":"64ccff39efe0a2737c8a320f8ce81c20","grade":true,"grade_id":"cell-233b453dcaca9a7f","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-1-Task-4"]},"outputs":[],"source":["assert p_value is not None\n","assert p_value >= 0.5\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"vWLbPiGQqcVq","nbgrader":{"cell_type":"code","checksum":"0db66d2b2e518a4cd0ef5e26e172f391","grade":false,"grade_id":"cell-94d8bf4762958fd4","locked":true,"schema_version":3,"solution":false}},"outputs":[],"source":["### RUN THIS CELL\n","plt.title('Distribution of Residuals')\n","plt.hist(residuals)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"iRQ93L5QwXQv","nbgrader":{"cell_type":"markdown","checksum":"4a7ac043d69e12e464f031f347c50e11","grade":false,"grade_id":"cell-4223fc25a791cf18","locked":true,"schema_version":3,"solution":false}},"source":["## Train Test Split"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"SpFPYbbFxKAm","nbgrader":{"cell_type":"code","checksum":"262c6caad8ae29e13c7143467b9904a1","grade":false,"grade_id":"cell-c809c8eeb82bb349","locked":true,"schema_version":3,"solution":false}},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"7FZS7zOnxKAy","nbgrader":{"cell_type":"markdown","checksum":"2625eceb6042604cca26e4bedaab7e44","grade":false,"grade_id":"cell-fdf5498ae35bd975","locked":true,"schema_version":3,"solution":false}},"source":["## Part 2: Implementation using Sklearn\n","\n","In this section, you will implement and validate linear regression using sklearn. You will use the easy and quick interface of sklearn to try out models with different combinations of the features and select the best combination of features using the Adjusted $R^2$ score."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"bed8MTVQxKAy","nbgrader":{"cell_type":"markdown","checksum":"d72546edd5a4953327da092a30e832a4","grade":false,"grade_id":"cell-f2ac39f2c3a8829f","locked":true,"schema_version":3,"solution":false}},"source":["### Exercise 2.1: Adjusted $R^2$\n","\n","**<div style=\"text-align: right\"> [POINTS: 1]</div>**\n","\n","The sklearn library has a number of functions to calculate different performance metrics but it misses out on the adjusted $R^2$ score. So in this section you will implement the function `adjusted_r2_score` that computes adjusted $R^2$ score for a model.\n","\n","Recall: $$\\text{Adjusted}\\ {R^2} = 1- (1-R^2)\\frac{n-1}{n-d-1}$$\n","\n","\\\n","\n","**Tasks:** Complete the function `adjusted_r2_score` by computing the adjusted $R^2$ score.\n","\n","* Compute the $R^2$ score using the `r2_score` function from sklearn and store it in `r2`.\n","\n","* Compute the adjusted $R^2$ using the previously calculated $R2$ and store it in `adj_r2`."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"BatxD37mxKAz","nbgrader":{"cell_type":"code","checksum":"94b040583210d82c043741a8c23eedad","grade":false,"grade_id":"cell-83ee2dfb3f1ee209","locked":false,"schema_version":3,"solution":true},"tags":["Ex-2-Task-1"]},"outputs":[],"source":["### Ex-2-Task-1\n","\n","def adjusted_r2_score(y_test, y_pred):\n","    n = X.shape[0]\n","    d = X.shape[1]\n","\n","    r2 = None\n","    adj_r2 = None\n","\n","    ### BEGIN SOLUTION\n","    # your code here\n","    raise NotImplementedError\n","    ### END SOLUTION\n","\n","    return adj_r2"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"Ty4lY9leBYs_","nbgrader":{"cell_type":"code","checksum":"a6e40d860489b178946dae6772a5865f","grade":true,"grade_id":"cell-8f71111613fd9599","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-2-Task-1"]},"outputs":[],"source":["# Tests\n","np.random.seed(42)\n","y_test_test = np.random.randn(12,1)\n","y_pred_test = y_test_test + 1\n","\n","assert (adjusted_r2_score(y_test_test,y_pred_test)) <= 0\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"E_sv93VHxKA2","nbgrader":{"cell_type":"markdown","checksum":"d459af76fc5ffab7e5607ac95d6d4d29","grade":false,"grade_id":"cell-fee8a8d6f760f873","locked":true,"schema_version":3,"solution":false}},"source":["### Exercise 2.2: Linear Regression using sklearn\n","\n","<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n","\n","In this exercise, you will use the [`LinearRegression`]() class from sklearn to build a number of linear regression models. Each model will use a particular combination of the available features. You will calculate the adjusted $R^2$ score of each of these models and see which particular combination of the features is the best.\n","\n","\n","**Tasks:**\n","\n","* Instantiate an object `linear_regression` of the `LinearRegression` class.\n","\n","* Fit the object `linear_regression` on the temporary training set `X_train_temp`.\n","\n","* Use the fitted `linear_regression` object to predict the output of the temporary test set `X_test_temp`. Store the predicted values in the variable `y_pred`.\n","\n","* Calculate the adjusted $R^2$ score for the model using the `adjusted_r2_score` you defined earlier and store it in `adj_r2`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"nndNjbharMt7","nbgrader":{"cell_type":"code","checksum":"ac5f108648b7359dd13e8559e48b0098","grade":false,"grade_id":"cell-4ab5ce07c89e83c4","locked":false,"schema_version":3,"solution":true},"tags":["Ex-2-Task-2"]},"outputs":[],"source":["### Ex-2-Task-2\n","\n","# Creating random combinations of the features\n","features_combinations = []\n","for L in range(1, X.shape[1]+1):\n","    for subset in itertools.combinations(range(X.shape[1]), L):\n","        features_combinations.append(list(subset))\n","\n","models = dict() # dictionary to store the parameters of different models\n","models_r2 = dict() # dictionary to store the different model's adjusted r2 score\n","\n","for features in features_combinations:\n","    X_train_temp = X_train[:, features]\n","    X_test_temp = X_test[:, features]\n","\n","    linear_regression = None\n","    y_pred = None\n","    adj_r2 = None\n","\n","    ### BEGIN SOLUTION\n","    # your code here\n","    raise NotImplementedError\n","    ### END SOLUTION\n","\n","    models[str(features)] = np.c_[linear_regression.intercept_, linear_regression.coef_]\n","    models_r2[str(features)] = adj_r2\n","\n","print(\"Different linear regression models and their adjusted r2 score\")\n","for key in models_r2.keys():\n","  print(\"{}: {}\".format(key, models_r2[key]))\n","best = max(models_r2, key=models_r2.get)\n","print(\"Best model: {} Parameters: {}\".format(best, models[best]))"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"kjerfk4mKPe-","nbgrader":{"cell_type":"code","checksum":"126965a96ebc6b873eedfdb6c5b9a962","grade":true,"grade_id":"cell-64a0f3111f3016b7","locked":true,"points":2,"schema_version":3,"solution":false},"tags":["Ex-2-Task-2"]},"outputs":[],"source":["assert adj_r2 is not None\n","assert round(adj_r2, 2) == 0.97\n","\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"fGNKpeH0yCCv","nbgrader":{"cell_type":"markdown","checksum":"191c01c6c94656f68d2a035b449aa85e","grade":false,"grade_id":"cell-9587536ae4d8e81a","locked":true,"schema_version":3,"solution":false}},"source":["### Exercise 2.3: Ridge Regression using sklearn\n","\n","<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n","\n","In this exercise, you will use the [`Ridge`]() class from sklearn to build a ridge regression model. You will use all 4 features as you have seen that they result in the best adjusted $R^2$.\n","\n","**Tasks:**\n","\n","* Import the  `Ridge` class from sklearn and instantiate the class as `ridge_regression`.\n","\n","* Fit the object `rige_regression` on the  training set `X_train`.\n","\n","* Use the fitted `rige_regression` object to predict the output of the  test set `X_test`. Store the predicted values in the variable `y_pred`.\n","\n","* Calculate the $R^2$ score for the model using the [`r2_score`]() function from sklearn and store it in `r2`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"kr64wmJyutUN","nbgrader":{"cell_type":"code","checksum":"51e4b9f2a20ed3390f8efd0e255ece66","grade":false,"grade_id":"cell-3f77efddb56dd281","locked":false,"schema_version":3,"solution":true},"tags":["Ex-2-Task-3"]},"outputs":[],"source":["### Ex-2-Task-3\n","\n","ridge_regression = None\n","y_pred = None\n","r2 = None\n","\n","### BEGIN SOLUTION\n","# your code here\n","raise NotImplementedError\n","### END SOLUTION\n","\n","print(\"R2 score for Ridge Regression: \",r2)\n","print(\"Parameters: \", np.c_[ridge_regression.intercept_, ridge_regression.coef_])"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"SmKOa4CmLaiN","nbgrader":{"cell_type":"code","checksum":"c551bfec5f52873e5dbb6652f8c8b218","grade":true,"grade_id":"cell-2d9c8e4368cf4997","locked":true,"points":2,"schema_version":3,"solution":false},"tags":["Ex-2-Task-3"]},"outputs":[],"source":["assert round(r2, 2) == 0.97\n","assert hasattr(ridge_regression, 'intercept_')\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"9lJ7KCvA6Aw1","nbgrader":{"cell_type":"markdown","checksum":"101c0e9b6fa730d4973254b1973dc385","grade":false,"grade_id":"cell-b168d1d64e24a12d","locked":true,"schema_version":3,"solution":false}},"source":["### Exercise 2.4: LASSO Regression using sklearn\n","\n","<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n","\n","In this exercise, you will use the [`Lasso`]() class from sklearn to build a lasso regression model. You will use all 4 features as you have seen that they result in the best adjusted $R^2$.\n","\n","**Tasks:**\n","\n","* Import the  `Lasso` class from sklearn and instantiate the class as `lasso_regression`\n","\n","* Fit the object `lasso_regression` on the temporary training set `X_train_temp`.\n","\n","* Use the fitted `lasso_regression` object to predict the output of the temporary test set `X_test_temp`. Store the predicted values in the variable `y_pred`.\n","\n","* Calculate the $R^2$ score for the model using the [`r2_score`]() function from sklearn and store it in `r2`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"OPMOGp05vnNW","nbgrader":{"cell_type":"code","checksum":"47e74eb62391139418bf6c6511177d47","grade":false,"grade_id":"cell-d672323a08606f16","locked":false,"schema_version":3,"solution":true},"tags":["Ex-2-Task-4"]},"outputs":[],"source":["### Ex-2-Task-4\n","\n","lasso_regression = None\n","y_pred = None\n","r2 = None\n","\n","### BEGIN SOLUTION\n","# your code here\n","raise NotImplementedError\n","### END SOLUTION\n","\n","print(\"R2 score for Lasso Regression: \",r2)\n","print(\"Parameters: \", np.r_[lasso_regression.intercept_, lasso_regression.coef_])\n","# lasso.coef_\n","# np.r_[lasso.intercept_, lasso.coef_]"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"EznfAsFINxae","nbgrader":{"cell_type":"code","checksum":"254a4f23043c66c7994096504ca18831","grade":true,"grade_id":"cell-6d73ade4caeaf8d3","locked":true,"points":2,"schema_version":3,"solution":false},"tags":["Ex-2-Task-4"]},"outputs":[],"source":["assert round(r2, 2) == 0.96\n","assert hasattr(lasso_regression, 'intercept_')\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"iiSf0Wdm7Lqj","nbgrader":{"cell_type":"markdown","checksum":"def85e4825c0caae5b8b8920c7559f47","grade":false,"grade_id":"cell-26f8d24d8dcbb9ea","locked":true,"schema_version":3,"solution":false}},"source":["## Part 3: Implementation from Scratch\n","\n","In this section, you will implement the OLS and Gradient descent algorithm from scratch to learn the parameters of the linear regression model. But first let's add the column of ones for intercept in the training and test set."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"1vAtvcmC-oGg","nbgrader":{"cell_type":"code","checksum":"ce98ac42b4d7c3d7360ed20e73d2cef4","grade":false,"grade_id":"cell-b7b8c90bedcf1369","locked":true,"schema_version":3,"solution":false}},"outputs":[],"source":["### RUN THIS CELL\n","X_train = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n","X_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"kN9ekI1lrA5b","nbgrader":{"cell_type":"markdown","checksum":"4033495d1a4d7a09615ad392e175f407","grade":false,"grade_id":"cell-6545ab12cd16e9a8","locked":true,"schema_version":3,"solution":false}},"source":["### Exercise 3.1: Linear Regression using OLS\n","\n","**<div style=\"text-align: right\"> [POINTS: 1]</div>**\n","\n","In this exercise, you will implement the normal equation of OLS to find the parameters of Linear regression.\n","\n","Recall: $$\\boldsymbol{\\beta} =(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}$$\n","\n","\\\\\n","\n","**Task:**\n","\n","* Complete the function `normal_equation` by implementing the normal equation to find the parameters `betas`."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"rpSF1M03sV5N","nbgrader":{"cell_type":"code","checksum":"babde6addc24a97afe8f906ebd29afb4","grade":false,"grade_id":"cell-46e37415db06d702","locked":false,"schema_version":3,"solution":true},"tags":["Ex-3-Task-1"]},"outputs":[],"source":["### Ex-3-Task-1\n","def normal_equation(X,y):\n","\n","    betas = None\n","    ### BEGIN SOLUTION\n","    # your code here\n","    raise NotImplementedError\n","    ### END SOLUTION\n","\n","    return betas\n","\n","model_OLS = normal_equation(X_train,y_train)\n","print(\"Parameters: \", model_OLS.reshape(1,-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"-bhOOQAqUTKL","nbgrader":{"cell_type":"code","checksum":"20dbfd11ad00726f2c6d20c2fc7a9504","grade":true,"grade_id":"cell-a5360b24d4331c58","locked":true,"points":1,"schema_version":3,"solution":false},"tags":["Ex-3-Task-1"]},"outputs":[],"source":["assert max(model_OLS) >=50"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"uWdzcJSJmlXG","nbgrader":{"cell_type":"markdown","checksum":"3148361bb7886b88c654215823b290ca","grade":false,"grade_id":"cell-97b1af6522db2f3c","locked":true,"schema_version":3,"solution":false}},"source":["### Exercise 3.2: Linear Regression using Gradient Descent\n","**<div style=\"text-align: right\"> [POINTS: 3]</div>**\n","\n","In this exercise, you will mplement the gradient descent algorithm to find the parameters of linear regression.\n","\n","**Tasks:** Complete the following function to implement the gradient descent algorithm.\n","\n","* Initialize `betas` randomly from values sampled from normal distribution\n","\n","* Calculate the `gradients` for the `betas`. $[\\frac{\\boldsymbol{\\partial J}}{\\boldsymbol{\\partial \\beta}}= \\mathbf{X^T}(\\mathbf{\\hat{y}-y})]$\n","\n","* Update the `betas` using their `gradients`.$[\\boldsymbol{\\beta} := \\boldsymbol{\\beta} - \\alpha \\boldsymbol{\\frac{\\partial J}{\\partial \\beta}}]$\n","\n","* Calculate the cost for the `betas` using the Linear Regression's cost function. $[J(\\boldsymbol{\\beta}) = \\frac{1}{2}\\ \\sum(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})^2]$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"CX5--5EQedbV","nbgrader":{"cell_type":"code","checksum":"764f9a1cdd1f17e08b66533f0d43e20d","grade":false,"grade_id":"cell-5a69d73a48281700","locked":false,"schema_version":3,"solution":true},"tags":["Ex-3-Task-2"]},"outputs":[],"source":["### Ex-3-Task-2\n","def gradient_descent(X, y, alpha=0.0001 , max_iters=10000, precision = 1e-3):\n","    np.random.seed(0)\n","    n = X.shape[0]\n","    d = X.shape[1]\n","\n","    iteration = 0 # no. of iterations\n","    difference = 1 # difference between the cost of current iteration and previous iteration\n","    costs = [1e12] # list containing the history of costs for different iterations\n","\n","    betas = None\n","    gradients = None\n","    cost = None\n","\n","    # Initialize betas\n","\n","    betas = np.random.randn(d,1)\n","\n","\n","    while difference > precision and iteration <= max_iters :\n","\n","        # Calculate gradients\n","\n","        # Update betas\n","\n","         # Calculate cost\n","\n","        ### BEGIN SOLUTION\n","        # your code here\n","        raise NotImplementedError\n","        ### END SOLUTION\n","\n","        difference = np.abs(costs[iteration] - cost)\n","        costs.append(cost)\n","\n","        iteration += 1\n","\n","        if(cost == np.infty):\n","            print(\"Cost reached infinity, try smaller learning rate\")\n","            break\n","\n","    return betas, iteration, costs\n","\n","model_GD, steps, costs = gradient_descent(X_train, y_train, alpha=0.0007)\n","print(\"Parameters: \", model_GD.reshape(1,-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"3PWr7HG7a7iy","nbgrader":{"cell_type":"code","checksum":"159ada235671e383b326a0d090a758ee","grade":true,"grade_id":"cell-b499e2072401f33b","locked":true,"points":3,"schema_version":3,"solution":false},"tags":["Ex-3-Task-2"]},"outputs":[],"source":["assert steps >=41\n","assert min(costs) < 1150"]},{"cell_type":"markdown","metadata":{"id":"EZlgJ4YU36xP"},"source":["## Well done!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9NLgoN1jm5s"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}